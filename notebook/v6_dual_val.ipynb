{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b202e7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import optuna\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.optimize import minimize as sp_minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa8a7d",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test_for_participants.csv'\n",
    "sample_path = '../data/sample_submission.csv'\n",
    "\n",
    "# â”€â”€ Dual Validation Date Boundaries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Val Set 1 (Physics): Autumn/Winter 2024 â€” cold, dark, windy grid\n",
    "VAL_PHYSICS_START = '2024-09-01'\n",
    "VAL_PHYSICS_END   = '2025-01-01'\n",
    "# Val Set 2 (Recency): Summer 2025 â€” last 3 months before LB\n",
    "VAL_RECENCY_START = '2025-06-01'\n",
    "\n",
    "SEED = 42\n",
    "N_TRIALS_LGB = 30\n",
    "SAVED_LGB_PATH = '../models/lgb_final.txt'\n",
    "ROUND_MULTIPLIER = 1.15\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd00b1c",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(train_path)\n",
    "test_raw = pd.read_csv(test_path)\n",
    "sample_sub = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_raw, test_raw]:\n",
    "    df['delivery_start'] = pd.to_datetime(df['delivery_start'])\n",
    "    df['delivery_end'] = pd.to_datetime(df['delivery_end'])\n",
    "\n",
    "train_raw['is_test'] = 0\n",
    "test_raw['is_test'] = 1\n",
    "test_raw['target'] = np.nan\n",
    "\n",
    "df = pd.concat([train_raw, test_raw], ignore_index=True)\n",
    "df = df.sort_values(['market', 'delivery_start']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2c5d5",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering - Ultra Advanced Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83988d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic time features\n",
    "ds = df[\"delivery_start\"]\n",
    "df[\"hour\"]         = ds.dt.hour\n",
    "df[\"day_of_week\"]  = ds.dt.dayofweek\n",
    "df[\"day_of_month\"] = ds.dt.day\n",
    "df[\"month\"]        = ds.dt.month\n",
    "df[\"quarter\"]      = ds.dt.quarter\n",
    "df[\"day_of_year\"]  = ds.dt.dayofyear\n",
    "df[\"year\"]         = ds.dt.year\n",
    "df[\"is_weekend\"]   = (ds.dt.dayofweek >= 5).astype(np.int8)\n",
    "df[\"week_of_year\"] = ds.dt.isocalendar().week.astype(int)\n",
    "\n",
    "# Cyclical encoding for time features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Market encoding\n",
    "market_map = {f\"Market {c}\": i for i, c in enumerate(\"ABCDEF\")}\n",
    "df[\"market_id\"] = df[\"market\"].map(market_map).astype(np.int8)\n",
    "\n",
    "# Advanced demand and supply features\n",
    "df[\"residual_demand\"] = df[\"load_forecast\"] - df[\"solar_forecast\"] - df[\"wind_forecast\"]\n",
    "df[\"supply_ratio\"] = (df[\"solar_forecast\"] + df[\"wind_forecast\"]) / (df[\"load_forecast\"] + 1)\n",
    "df[\"renewable_ratio\"] = (df[\"solar_forecast\"] + df[\"wind_forecast\"]) / (df[\"solar_forecast\"] + df[\"wind_forecast\"] + df[\"load_forecast\"] + 1)\n",
    "df[\"net_supply\"] = df[\"solar_forecast\"] + df[\"wind_forecast\"]\n",
    "df[\"demand_supply_balance\"] = df[\"load_forecast\"] / (df[\"solar_forecast\"] + df[\"wind_forecast\"] + 1)\n",
    "\n",
    "# Tightness ratios\n",
    "df[\"tightness_ratio\"] = df[\"residual_demand\"] / (df[\"load_forecast\"] + 1)\n",
    "df[\"tightness_x_month\"] = df[\"tightness_ratio\"] * df[\"month\"]\n",
    "df[\"tightness_x_hour\"] = df[\"tightness_ratio\"] * df[\"hour\"]\n",
    "df[\"tightness_x_dow\"] = df[\"tightness_ratio\"] * df[\"day_of_week\"]\n",
    "\n",
    "# Price sensitivity indicators\n",
    "df[\"solar_wind_ratio\"] = df[\"solar_forecast\"] / (df[\"wind_forecast\"] + 1)\n",
    "df[\"wind_solar_ratio\"] = df[\"wind_forecast\"] / (df[\"solar_forecast\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_weather_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Advanced Weather Physics Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# First, alias columns whose names differ between dataset and our code\n",
    "\n",
    "if 'convective_available_potential_energy' in df.columns:\n",
    "    df['cape'] = df['convective_available_potential_energy']\n",
    "if 'precipitation_amount' in df.columns:\n",
    "    df['precipitation'] = df['precipitation_amount']\n",
    "if 'apparent_temperature_2m' in df.columns:\n",
    "    df['apparent_temperature'] = df['apparent_temperature_2m']\n",
    "if 'freezing_level_height' in df.columns:\n",
    "    df['boundary_layer_height'] = df['freezing_level_height']\n",
    "\n",
    "# Estimate missing columns from available physics\n",
    "# Saturation vapour pressure (Tetens formula)\n",
    "es = 6.112 * np.exp((17.67 * df['air_temperature_2m']) / (df['air_temperature_2m'] + 243.5))\n",
    "ea = (df['relative_humidity_2m'] / 100.0) * es\n",
    "df['vapour_pressure_deficit_2m'] = es - ea\n",
    "\n",
    "# Proxy precipitation probability from relative humidity\n",
    "df['precipitation_probability'] = np.where(df['relative_humidity_2m'] > 85, 50, 0)\n",
    "\n",
    "# â”€â”€ Derived weather features â”€â”€\n",
    "df['dew_point_depression']  = df['air_temperature_2m'] - df['dew_point_temperature_2m']\n",
    "df['wet_bulb_depression']   = df['air_temperature_2m'] - df['wet_bulb_temperature_2m']\n",
    "df['humidity_ratio']        = (0.622 * df['vapour_pressure_deficit_2m']) / (df['surface_pressure'] - df['vapour_pressure_deficit_2m'])\n",
    "df['blh_normalized_pressure'] = df['boundary_layer_height'] / (df['surface_pressure'] / 1000)\n",
    "\n",
    "# Wind shear (10m vs 80m)\n",
    "df['wind_shear']       = df['wind_speed_80m'] - df['wind_speed_10m']\n",
    "df['wind_shear_ratio'] = df['wind_speed_80m'] / (df['wind_speed_10m'] + 0.1)\n",
    "\n",
    "# Convection indices\n",
    "df['cape_cin_interaction'] = df['cape'] * df['convective_inhibition']\n",
    "df['convection_potential'] = df['cape'] / (abs(df['convective_inhibition']) + 1)\n",
    "\n",
    "# Visibility & cloud\n",
    "df['visibility_cloud_interaction'] = df['visibility'] / (df['cloud_cover_total'] + 1)\n",
    "\n",
    "# Combined weather severity index\n",
    "df['weather_severity'] = (\n",
    "    df['cloud_cover_total'] / 100 +\n",
    "    (100 - df['visibility'].clip(0, 100)) / 100 +\n",
    "    df['precipitation_probability'] / 100 +\n",
    "    df['cape'] / 1000\n",
    ") / 4\n",
    "\n",
    "# Solar / wind potential\n",
    "df['solar_potential'] = df['global_horizontal_irradiance'] * (1 - df['cloud_cover_total'] / 100)\n",
    "df['wind_potential']  = df['wind_speed_80m'] ** 3  # cubic âˆ power\n",
    "\n",
    "# Extreme weather flags\n",
    "df['extreme_temp']   = ((df['air_temperature_2m'] > 30) | (df['air_temperature_2m'] < -5)).astype(int)\n",
    "df['extreme_wind']   = (df['wind_speed_80m'] > 25).astype(int)\n",
    "df['extreme_precip'] = (df['precipitation'] > 5).astype(int)\n",
    "\n",
    "# Seasonal Ã— weather interactions\n",
    "df['temp_month_interaction'] = df['air_temperature_2m'] * df['month']\n",
    "df['wind_month_interaction'] = df['wind_speed_80m'] * df['month']\n",
    "df['solar_hour_interaction'] = df['solar_forecast'] * df['hour']\n",
    "\n",
    "# Heating / cooling degree-hours\n",
    "df['cooling_degree_hours'] = np.maximum(df['air_temperature_2m'] - 22, 0)\n",
    "df['heating_degree_hours'] = np.maximum(18 - df['air_temperature_2m'], 0)\n",
    "\n",
    "# Vapour pressure deficit normalised\n",
    "df['vpd_normalized'] = df['vapour_pressure_deficit_2m'] / df['surface_pressure']\n",
    "\n",
    "# Apparent temperature anomaly\n",
    "df['apparent_temp_anomaly']   = df['apparent_temperature'] - df['air_temperature_2m']\n",
    "df['apparent_air_temp_ratio'] = df['apparent_temperature'] / (df['air_temperature_2m'] + 1)\n",
    "\n",
    "# Lifted Index feature (strong storm predictor)\n",
    "if 'lifted_index' in df.columns:\n",
    "    df['lifted_index_negative'] = (-df['lifted_index']).clip(lower=0)  # only instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "momentum_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Weather Momentum & Lag Features (NO target leakage) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "weather_cols = ['wind_speed_80m', 'solar_forecast', 'load_forecast',\n",
    "                'wind_forecast', 'air_temperature_2m']\n",
    "\n",
    "for col in weather_cols:\n",
    "    grp = df.groupby('market_id')[col]\n",
    "\n",
    "    # Hourly differences (momentum)\n",
    "    df[f'{col}_diff_1h']  = grp.diff(1)\n",
    "    df[f'{col}_diff_3h']  = grp.diff(3)\n",
    "    df[f'{col}_diff_6h']  = grp.diff(6)\n",
    "    df[f'{col}_diff_12h'] = grp.diff(12)\n",
    "\n",
    "    # Rolling mean / std\n",
    "    df[f'{col}_rolling_mean_6h']  = grp.transform(lambda x: x.rolling(6,  min_periods=1).mean())\n",
    "    df[f'{col}_rolling_std_6h']   = grp.transform(lambda x: x.rolling(6,  min_periods=1).std().fillna(0))\n",
    "    df[f'{col}_rolling_mean_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).mean())\n",
    "    df[f'{col}_rolling_std_24h']  = grp.transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "\n",
    "    # Rolling min / max (use bfill() instead of deprecated fillna(method=...))\n",
    "    df[f'{col}_rolling_min_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).min().bfill())\n",
    "    df[f'{col}_rolling_max_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).max().bfill())\n",
    "    df[f'{col}_range_24h']       = df[f'{col}_rolling_max_24h'] - df[f'{col}_rolling_min_24h']\n",
    "\n",
    "    # Exponential weighted moving averages\n",
    "    df[f'{col}_ewm_6h']  = grp.transform(lambda x: x.ewm(span=6,  adjust=False).mean())\n",
    "    df[f'{col}_ewm_24h'] = grp.transform(lambda x: x.ewm(span=24, adjust=False).mean())\n",
    "\n",
    "    # Z-score vs rolling window\n",
    "    df[f'{col}_zscore_24h'] = (df[col] - df[f'{col}_rolling_mean_24h']) / (df[f'{col}_rolling_std_24h'] + 0.001)\n",
    "\n",
    "# Temperature anomaly vs recent history\n",
    "df['temp_24h_mean']    = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(24, min_periods=1).mean())\n",
    "df['temp_72h_mean']    = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(72, min_periods=1).mean())\n",
    "df['temp_anomaly_24h'] = df['air_temperature_2m'] - df['temp_24h_mean']\n",
    "df['temp_anomaly_72h'] = df['air_temperature_2m'] - df['temp_72h_mean']\n",
    "\n",
    "# Wind direction components & stability\n",
    "df['wind_dir_sin']    = np.sin(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_dir_cos']    = np.cos(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_dir_change'] = df.groupby('market_id')['wind_direction_80m'].diff(1).abs()\n",
    "\n",
    "# Pressure & humidity interactions with temperature\n",
    "df['pressure_temp_interaction']  = df['surface_pressure'] * df['air_temperature_2m']\n",
    "df['humidity_temp_interaction']  = df['relative_humidity_2m'] * df['air_temperature_2m']\n",
    "df['pressure_gradient']          = df.groupby('market_id')['surface_pressure'].diff(1)\n",
    "\n",
    "# Cloud & precipitation transforms\n",
    "df['cloud_cover_total_sq']  = df['cloud_cover_total'] ** 2\n",
    "df['cloud_cover_effect']    = df['cloud_cover_total'] * df['global_horizontal_irradiance']\n",
    "df['precip_prob_sq']        = df['precipitation_probability'] ** 2\n",
    "df['precip_effect']         = df['precipitation'] * df['precipitation_probability']\n",
    "\n",
    "# Radiation efficiency\n",
    "df['solar_efficiency']       = df['solar_forecast'] / (df['global_horizontal_irradiance'] + 1)\n",
    "df['radiation_cloud_ratio']  = df['global_horizontal_irradiance'] / (df['cloud_cover_total'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced temporal features\n",
    "\n",
    "# Hourly and daily patterns\n",
    "df['hour_from_peak'] = abs(df['hour'] - 12)  # Distance from peak solar hour\n",
    "df['is_peak_solar'] = ((df['hour'] >= 10) & (df['hour'] <= 16)).astype(int)\n",
    "df['is_off_peak'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "df['is_business_hours'] = ((df['hour'] >= 8) & (df['hour'] <= 18) & (df['day_of_week'] < 5)).astype(int)\n",
    "\n",
    "# Week patterns\n",
    "df['is_monday'] = (df['day_of_week'] == 0).astype(int)\n",
    "df['is_friday'] = (df['day_of_week'] == 4).astype(int)\n",
    "df['is_weekend_start'] = (df['day_of_week'] == 4).astype(int)  # Friday\n",
    "df['is_weekend_end'] = (df['day_of_week'] == 6).astype(int)    # Sunday\n",
    "\n",
    "# Monthly patterns\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "df['is_autumn'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "\n",
    "# Quarter interactions\n",
    "df['q1_temp_interaction'] = (df['quarter'] == 1) * df['air_temperature_2m']\n",
    "df['q2_temp_interaction'] = (df['quarter'] == 2) * df['air_temperature_2m']\n",
    "df['q3_temp_interaction'] = (df['quarter'] == 3) * df['air_temperature_2m']\n",
    "df['q4_temp_interaction'] = (df['quarter'] == 4) * df['air_temperature_2m']\n",
    "\n",
    "# Seasonal demand patterns\n",
    "df['winter_load_factor'] = df['is_winter'] * df['load_forecast']\n",
    "df['summer_load_factor'] = df['is_summer'] * df['load_forecast']\n",
    "df['spring_load_factor'] = df['is_spring'] * df['load_forecast']\n",
    "df['autumn_load_factor'] = df['is_autumn'] * df['load_forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Historical Target Encoding  (VALIDATION-SAFE MODE â€” v6 Dual Val)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# For tuning/validation: compute group means ONLY from TRAINING data\n",
    "# (excluding both validation sets) to avoid any leakage.\n",
    "\n",
    "# âš ï¸ VALIDATION MODE: using only training data (pre-Physics val start)\n",
    "# For final submission, change to:\n",
    "#     strict_train = df[(df['is_test'] == 0) & (df['delivery_start'] < VAL_PHYSICS_START)]\n",
    "strict_train = df[(df['is_test'] == 0) & (df['delivery_start'] < VAL_PHYSICS_START)]\n",
    "\n",
    "# â”€â”€ Market Ã— Hour mean â”€â”€\n",
    "mean_mh = (strict_train\n",
    "    .groupby(['market_id', 'hour'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mh'))\n",
    "\n",
    "# â”€â”€ Market Ã— Day-of-week mean â”€â”€\n",
    "mean_mdow = (strict_train\n",
    "    .groupby(['market_id', 'day_of_week'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mdow'))\n",
    "\n",
    "# â”€â”€ Market Ã— Month mean â”€â”€\n",
    "mean_mm = (strict_train\n",
    "    .groupby(['market_id', 'month'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mm'))\n",
    "\n",
    "# â”€â”€ Market mean (global baseline per market) â”€â”€\n",
    "mean_m = (strict_train\n",
    "    .groupby(['market_id'])['target']\n",
    "    .mean().reset_index(name='target_histmean_m'))\n",
    "\n",
    "# â”€â”€ Hour mean (global baseline per hour) â”€â”€\n",
    "mean_h = (strict_train\n",
    "    .groupby(['hour'])['target']\n",
    "    .mean().reset_index(name='target_histmean_h'))\n",
    "\n",
    "# â”€â”€ Market Ã— Hour Ã— DayOfWeek mean â”€â”€\n",
    "mean_mhd = (strict_train\n",
    "    .groupby(['market_id', 'hour', 'day_of_week'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mhd'))\n",
    "\n",
    "# â”€â”€ Market Ã— Quarter mean â”€â”€\n",
    "mean_mq = (strict_train\n",
    "    .groupby(['market_id', 'quarter'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mq'))\n",
    "\n",
    "# Merge ALL onto full dataframe\n",
    "df = df.merge(mean_mh,  on=['market_id', 'hour'],          how='left')\n",
    "df = df.merge(mean_mdow, on=['market_id', 'day_of_week'],  how='left')\n",
    "df = df.merge(mean_mm,  on=['market_id', 'month'],          how='left')\n",
    "df = df.merge(mean_m,   on=['market_id'],                   how='left')\n",
    "df = df.merge(mean_h,   on=['hour'],                        how='left')\n",
    "df = df.merge(mean_mhd, on=['market_id', 'hour', 'day_of_week'], how='left')\n",
    "df = df.merge(mean_mq,  on=['market_id', 'quarter'],       how='left')\n",
    "\n",
    "# Fill any NaN hist-means with global training mean\n",
    "global_mean = strict_train['target'].mean()\n",
    "for c in [c for c in df.columns if c.startswith('target_histmean_')]:\n",
    "    df[c] = df[c].fillna(global_mean)\n",
    "\n",
    "# â”€â”€ Deviations from historical baselines (computed from safe features only) â”€â”€\n",
    "df['histmean_mh_x_residual'] = df['target_histmean_mh'] * df['residual_demand']\n",
    "df['histmean_mh_x_tightness'] = df['target_histmean_mh'] * df['tightness_ratio']\n",
    "df['histmean_deviation_dow_vs_m'] = df['target_histmean_mdow'] - df['target_histmean_m']\n",
    "df['histmean_deviation_mh_vs_h'] = df['target_histmean_mh'] - df['target_histmean_h']\n",
    "\n",
    "print(f\"âœ… Safe historical encoding: {sum(c.startswith('target_histmean') for c in df.columns)} features\")\n",
    "print(f\"   Global training mean: {global_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interaction_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Interaction features (weather Ã— demand, cross-market) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['temp_load_interaction']  = df['air_temperature_2m'] * df['load_forecast']\n",
    "df['wind_load_interaction']  = df['wind_speed_80m'] * df['load_forecast']\n",
    "df['solar_load_interaction'] = df['solar_forecast'] * df['load_forecast']\n",
    "df['temp_wind_interaction']  = df['air_temperature_2m'] * df['wind_speed_80m']\n",
    "df['temp_solar_interaction'] = df['air_temperature_2m'] * df['solar_forecast']\n",
    "df['wind_solar_interaction'] = df['wind_speed_80m'] * df['solar_forecast']\n",
    "\n",
    "# Triple interactions\n",
    "df['temp_wind_load_interaction']  = df['air_temperature_2m'] * df['wind_speed_80m'] * df['load_forecast']\n",
    "df['temp_solar_load_interaction'] = df['air_temperature_2m'] * df['solar_forecast'] * df['load_forecast']\n",
    "df['wind_solar_load_interaction'] = df['wind_speed_80m'] * df['solar_forecast'] * df['load_forecast']\n",
    "\n",
    "# Weather volatility (rolling on weather â€” safe, no target)\n",
    "df['temp_volatility']  = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "df['wind_volatility']  = df.groupby('market_id')['wind_speed_80m'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "df['solar_volatility'] = df.groupby('market_id')['solar_forecast'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "\n",
    "# Rate of change (weather â€” safe)\n",
    "df['temp_rate_change']  = df.groupby('market_id')['air_temperature_2m'].diff(1) / (df.groupby('market_id')['air_temperature_2m'].shift(1).abs() + 0.01)\n",
    "df['wind_rate_change']  = df.groupby('market_id')['wind_speed_80m'].diff(1) / (df.groupby('market_id')['wind_speed_80m'].shift(1).abs() + 0.01)\n",
    "df['solar_rate_change'] = df.groupby('market_id')['solar_forecast'].diff(1) / (df.groupby('market_id')['solar_forecast'].shift(1).abs() + 0.01)\n",
    "\n",
    "# Cross-market features (same timestamp, across markets â€” safe)\n",
    "for col in ['wind_speed_80m', 'solar_forecast', 'load_forecast']:\n",
    "    ts_mean = df.groupby('delivery_start')[col].transform('mean')\n",
    "    ts_std  = df.groupby('delivery_start')[col].transform('std') + 0.001\n",
    "    df[f'{col}_market_diff']   = df[col] - ts_mean\n",
    "    df[f'{col}_market_zscore'] = (df[col] - ts_mean) / ts_std\n",
    "\n",
    "# Advanced rolling statistics (weather â€” safe)\n",
    "for col in ['wind_speed_80m', 'solar_forecast', 'load_forecast']:\n",
    "    df[f'{col}_skew_24h'] = df.groupby('market_id')[col].transform(\n",
    "        lambda x: x.rolling(24, min_periods=12).skew().fillna(0))\n",
    "\n",
    "\n",
    "# â”€â”€ SUMMER HEATWAVE & GRID STRESS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Heatwave Penalty (Exponential stress when Temp > 25C and Wind is low)\n",
    "df['heatwave_stress'] = np.where(\n",
    "    (df['air_temperature_2m'] > 25) & (df['wind_speed_80m'] < 5),\n",
    "    (df['air_temperature_2m'] - 25) ** 2, \n",
    "    0\n",
    ")\n",
    "\n",
    "# 2. Solar/Wind Drought Flag\n",
    "df['renewable_drought'] = ((df['solar_forecast'] < 10) & (df['wind_forecast'] < 10)).astype(int)\n",
    "\n",
    "# 3. Summer Cooling Load Proxy\n",
    "df['cooling_degree_load'] = df['load_forecast'] * np.maximum(0, df['air_temperature_2m'] - 22)\n",
    "\n",
    "# â”€â”€ Final NaN handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NEVER fill target NaN â€” those are test rows!\n",
    "exclude_from_fill = {'target', 'delivery_start', 'delivery_end', 'market', 'id'}\n",
    "for col in df.columns:\n",
    "    if col in exclude_from_fill:\n",
    "        continue\n",
    "    if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int8']:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(f\"âœ… Feature engineering complete: {len(df.columns)} total columns\")\n",
    "print(f\"   Training rows: {(df['is_test']==0).sum()}, Test rows: {(df['is_test']==1).sum()}\")\n",
    "print(f\"   NaN check (target): {df['target'].isna().sum()} NaN (should equal test rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8cf65",
   "metadata": {},
   "source": [
    "## Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc058ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_df = df[df['is_test'] == 0].copy()\n",
    "test_df = df[df['is_test'] == 1].copy()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DUAL VALIDATION SPLITTING LOGIC\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. Val Set 1: The \"Physics\" Test (Autumn/Winter 2024)\n",
    "mask_val_physics = (observed_df['delivery_start'] >= VAL_PHYSICS_START) & (observed_df['delivery_start'] < VAL_PHYSICS_END)\n",
    "\n",
    "# 2. Val Set 2: The \"Recency\" Test (Summer 2025 â€” last 3 months)\n",
    "mask_val_recency = (observed_df['delivery_start'] >= VAL_RECENCY_START)\n",
    "\n",
    "# 3. Train Set: Everything else\n",
    "mask_train = ~(mask_val_physics | mask_val_recency)\n",
    "\n",
    "train_df = observed_df[mask_train]\n",
    "val_physics_df = observed_df[mask_val_physics]\n",
    "val_recency_df = observed_df[mask_val_recency]\n",
    "\n",
    "print(f'ğŸŒ² Training rows: {len(train_df):,}')\n",
    "print(f'â„ï¸ Val 1 (Physics â€” Autumn/Winter 2024): {len(val_physics_df):,}')\n",
    "print(f'ğŸ”¥ Val 2 (Recency â€” Summer 2025):        {len(val_recency_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844edcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = set(['id', 'target', 'market', 'delivery_start', 'delivery_end', 'is_test'])\n",
    "feat_cols = sorted([c for c in df.columns if c not in drop_cols])\n",
    "cat_idx = [feat_cols.index('market_id')] if 'market_id' in feat_cols else []\n",
    "\n",
    "X_train = train_df[feat_cols]\n",
    "y_train_real = train_df['target'].values\n",
    "y_train = np.arcsinh(train_df['target'].values)\n",
    "\n",
    "X_val_physics = val_physics_df[feat_cols]\n",
    "y_val_physics_real = val_physics_df['target'].values\n",
    "y_val_physics = np.arcsinh(val_physics_df['target'].values)\n",
    "\n",
    "X_val_recency = val_recency_df[feat_cols]\n",
    "y_val_recency_real = val_recency_df['target'].values\n",
    "y_val_recency = np.arcsinh(val_recency_df['target'].values)\n",
    "\n",
    "X_all = observed_df[feat_cols]\n",
    "y_all_real = observed_df['target'].values\n",
    "y_all = np.arcsinh(observed_df['target'].values)\n",
    "\n",
    "X_test = test_df[feat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2534d",
   "metadata": {},
   "source": [
    "## Split-Train with Dual Validation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SPLIT-TRAIN: DUAL VALIDATION DASHBOARD\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#   - Market 0:    Time-decay weights (0.1â†’1.0)\n",
    "#   - Markets 1-5: Static baseline\n",
    "#   - DUAL VALIDATION: Physics (Winter 2024) + Recency (Summer 2025)\n",
    "#     LightGBM early-stops on the FIRST valid_set in the list.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# â”€â”€ Baseline Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "baseline_params = {\n",
    "    \"objective\": \"huber\", \"alpha\": 1.5, \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1, \"seed\": SEED, \"n_jobs\": -1,\n",
    "    \"max_depth\": 10, \"num_leaves\": 255, \"learning_rate\": 0.03,\n",
    "    \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1\n",
    "}\n",
    "\n",
    "# â”€â”€ Market-level masks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mask_train_m0   = X_train[\"market_id\"] == 0\n",
    "mask_train_rest = X_train[\"market_id\"] > 0\n",
    "mask_vp_m0   = X_val_physics[\"market_id\"] == 0\n",
    "mask_vp_rest = X_val_physics[\"market_id\"] > 0\n",
    "mask_vr_m0   = X_val_recency[\"market_id\"] == 0\n",
    "mask_vr_rest = X_val_recency[\"market_id\"] > 0\n",
    "\n",
    "X_train_m0, y_train_m0     = X_train[mask_train_m0], y_train[mask_train_m0.values]\n",
    "X_train_rest, y_train_rest = X_train[mask_train_rest], y_train[mask_train_rest.values]\n",
    "\n",
    "X_vp_m0,  y_vp_m0  = X_val_physics[mask_vp_m0],  y_val_physics[mask_vp_m0.values]\n",
    "X_vp_rest, y_vp_rest = X_val_physics[mask_vp_rest], y_val_physics[mask_vp_rest.values]\n",
    "y_vp_real_m0  = y_val_physics_real[mask_vp_m0.values]\n",
    "y_vp_real_rest = y_val_physics_real[mask_vp_rest.values]\n",
    "\n",
    "X_vr_m0,  y_vr_m0  = X_val_recency[mask_vr_m0],  y_val_recency[mask_vr_m0.values]\n",
    "X_vr_rest, y_vr_rest = X_val_recency[mask_vr_rest], y_val_recency[mask_vr_rest.values]\n",
    "y_vr_real_m0  = y_val_recency_real[mask_vr_m0.values]\n",
    "y_vr_real_rest = y_val_recency_real[mask_vr_rest.values]\n",
    "\n",
    "feat_cols_m0 = [c for c in feat_cols if c != \"market_id\"]\n",
    "\n",
    "print(f\"Market 0    â€” Train: {len(X_train_m0):,}  |  Physics: {len(X_vp_m0):,}  |  Recency: {len(X_vr_m0):,}\")\n",
    "print(f\"Markets 1-5 â€” Train: {len(X_train_rest):,}  |  Physics: {len(X_vp_rest):,}  |  Recency: {len(X_vr_rest):,}\")\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 1 â€” MODEL 0 (TIME-DECAY + DUAL VAL)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 65)\n",
    "print(\"ğŸ”· TRAINING MODEL 0 (Market 0 â€” Time-Decay + Dual Validation)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "weights_m0 = np.linspace(0.1, 1.0, len(X_train_m0))\n",
    "print(f\"   Time-decay weights: {weights_m0[0]:.2f} â†’ {weights_m0[-1]:.2f}\")\n",
    "\n",
    "baseline_params_m0 = {\n",
    "    \"objective\": \"huber\", \"alpha\": 1.5, \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1, \"seed\": SEED, \"n_jobs\": -1,\n",
    "    \"max_depth\": 12, \"num_leaves\": 512, \"learning_rate\": 0.03,\n",
    "    \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 1\n",
    "}\n",
    "\n",
    "ds_train_m0 = lgb.Dataset(X_train_m0[feat_cols_m0], y_train_m0, weight=weights_m0, free_raw_data=False)\n",
    "ds_vp_m0    = lgb.Dataset(X_vp_m0[feat_cols_m0], y_vp_m0, reference=ds_train_m0, free_raw_data=False)\n",
    "ds_vr_m0    = lgb.Dataset(X_vr_m0[feat_cols_m0], y_vr_m0, reference=ds_train_m0, free_raw_data=False)\n",
    "\n",
    "evals_result_m0 = {}\n",
    "model_m0 = lgb.train(\n",
    "    baseline_params_m0, ds_train_m0,\n",
    "    num_boost_round=4000,\n",
    "    valid_sets=[ds_vp_m0, ds_vr_m0],\n",
    "    valid_names=[\"val_physics\", \"val_recency\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(150, verbose=False),\n",
    "        lgb.log_evaluation(100),\n",
    "        lgb.record_evaluation(evals_result_m0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "vp_preds_m0_real = np.sinh(model_m0.predict(X_vp_m0[feat_cols_m0]))\n",
    "vr_preds_m0_real = np.sinh(model_m0.predict(X_vr_m0[feat_cols_m0]))\n",
    "rmse_m0_physics = root_mean_squared_error(y_vp_real_m0, vp_preds_m0_real)\n",
    "rmse_m0_recency = root_mean_squared_error(y_vr_real_m0, vr_preds_m0_real)\n",
    "print(f\"\\nğŸ”· M0 â€” Physics: {rmse_m0_physics:.4f}  |  Recency: {rmse_m0_recency:.4f}  |  Iters: {model_m0.best_iteration}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 2 â€” MODEL REST (STATIC + DUAL VAL)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"ğŸŸ¢ TRAINING MODEL REST (Markets 1-5 â€” Static + Dual Validation)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "ds_train_rest = lgb.Dataset(X_train_rest, y_train_rest, categorical_feature=cat_idx if cat_idx else \"auto\", free_raw_data=False)\n",
    "ds_vp_rest    = lgb.Dataset(X_vp_rest, y_vp_rest, reference=ds_train_rest, free_raw_data=False)\n",
    "ds_vr_rest    = lgb.Dataset(X_vr_rest, y_vr_rest, reference=ds_train_rest, free_raw_data=False)\n",
    "\n",
    "evals_result_rest = {}\n",
    "model_rest = lgb.train(\n",
    "    baseline_params, ds_train_rest,\n",
    "    num_boost_round=3000,\n",
    "    valid_sets=[ds_vp_rest, ds_vr_rest],\n",
    "    valid_names=[\"val_physics\", \"val_recency\"],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100, verbose=False),\n",
    "        lgb.log_evaluation(100),\n",
    "        lgb.record_evaluation(evals_result_rest),\n",
    "    ],\n",
    ")\n",
    "\n",
    "vp_preds_rest_real = np.sinh(model_rest.predict(X_vp_rest))\n",
    "vr_preds_rest_real = np.sinh(model_rest.predict(X_vr_rest))\n",
    "rmse_rest_physics = root_mean_squared_error(y_vp_real_rest, vp_preds_rest_real)\n",
    "rmse_rest_recency = root_mean_squared_error(y_vr_real_rest, vr_preds_rest_real)\n",
    "print(f\"\\nğŸŸ¢ MR â€” Physics: {rmse_rest_physics:.4f}  |  Recency: {rmse_rest_recency:.4f}  |  Iters: {model_rest.best_iteration}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 3 â€” COMBINED DASHBOARD\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "vp_combined = np.empty(len(y_val_physics_real))\n",
    "vp_combined[mask_vp_m0.values]   = vp_preds_m0_real\n",
    "vp_combined[mask_vp_rest.values] = vp_preds_rest_real\n",
    "combined_physics = root_mean_squared_error(y_val_physics_real, vp_combined)\n",
    "\n",
    "vr_combined = np.empty(len(y_val_recency_real))\n",
    "vr_combined[mask_vr_m0.values]   = vr_preds_m0_real\n",
    "vr_combined[mask_vr_rest.values] = vr_preds_rest_real\n",
    "combined_recency = root_mean_squared_error(y_val_recency_real, vr_combined)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"ğŸ“Š DUAL VALIDATION DASHBOARD\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  ğŸ”· Market 0    â€” Physics: {rmse_m0_physics:.4f}  |  Recency: {rmse_m0_recency:.4f}  ({model_m0.best_iteration} iters)\")\n",
    "print(f\"  ğŸŸ¢ Markets 1-5 â€” Physics: {rmse_rest_physics:.4f}  |  Recency: {rmse_rest_recency:.4f}  ({model_rest.best_iteration} iters)\")\n",
    "print(f\"  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "print(f\"  â„ï¸ COMBINED Physics   RMSE: {combined_physics:.4f}\")\n",
    "print(f\"  ğŸ”¥ COMBINED Recency   RMSE: {combined_recency:.4f}\")\n",
    "print(f\"  â± Total time: {elapsed:.0f}s\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a53578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Learning Curves â€” Dual Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "vl_p_m0 = evals_result_m0[\"val_physics\"][\"rmse\"]\n",
    "vl_r_m0 = evals_result_m0[\"val_recency\"][\"rmse\"]\n",
    "axes[0].plot(range(1, len(vl_p_m0)+1), vl_p_m0, label='Val Physics', alpha=0.8, lw=1.5)\n",
    "axes[0].plot(range(1, len(vl_r_m0)+1), vl_r_m0, label='Val Recency', alpha=0.8, lw=1.5, ls='--')\n",
    "axes[0].axvline(x=model_m0.best_iteration, color='red', ls='--', alpha=0.5, label=f'Best ({model_m0.best_iteration})')\n",
    "axes[0].set_xlabel('Iterations'); axes[0].set_ylabel('RMSE (arcsinh)')\n",
    "axes[0].set_title('ğŸ”· Model 0 â€” Dual Validation Curves')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "vl_p_rest = evals_result_rest[\"val_physics\"][\"rmse\"]\n",
    "vl_r_rest = evals_result_rest[\"val_recency\"][\"rmse\"]\n",
    "axes[1].plot(range(1, len(vl_p_rest)+1), vl_p_rest, label='Val Physics', alpha=0.8, lw=1.5)\n",
    "axes[1].plot(range(1, len(vl_r_rest)+1), vl_r_rest, label='Val Recency', alpha=0.8, lw=1.5, ls='--')\n",
    "axes[1].axvline(x=model_rest.best_iteration, color='red', ls='--', alpha=0.5, label=f'Best ({model_rest.best_iteration})')\n",
    "axes[1].set_xlabel('Iterations'); axes[1].set_ylabel('RMSE (arcsinh)')\n",
    "axes[1].set_title('ğŸŸ¢ Model Rest â€” Dual Validation Curves')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Per-Market RMSE â€” Physics Val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vp_res = val_physics_df[['delivery_start', 'market_id']].copy()\n",
    "vp_res['actual'] = y_val_physics_real\n",
    "vp_res['predicted'] = vp_combined\n",
    "\n",
    "print('\\n' + '=' * 65)\n",
    "print('â„ï¸ RMSE BY MARKET â€” PHYSICS VAL (Autumn/Winter 2024)')\n",
    "print('=' * 65)\n",
    "for mkt in sorted(vp_res['market_id'].unique()):\n",
    "    d = vp_res[vp_res['market_id'] == mkt]\n",
    "    tag = 'ğŸ”· M0' if mkt == 0 else 'ğŸŸ¢ MR'\n",
    "    print(f'  {tag} | Market {mkt} | Rows: {len(d):4d} | RMSE: {root_mean_squared_error(d[\"actual\"], d[\"predicted\"]):.4f}')\n",
    "print('=' * 65)\n",
    "\n",
    "# â”€â”€ Per-Market RMSE â€” Recency Val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vr_res = val_recency_df[['delivery_start', 'market_id']].copy()\n",
    "vr_res['actual'] = y_val_recency_real\n",
    "vr_res['predicted'] = vr_combined\n",
    "\n",
    "print('\\n' + '=' * 65)\n",
    "print('ğŸ”¥ RMSE BY MARKET â€” RECENCY VAL (Summer 2025)')\n",
    "print('=' * 65)\n",
    "for mkt in sorted(vr_res['market_id'].unique()):\n",
    "    d = vr_res[vr_res['market_id'] == mkt]\n",
    "    tag = 'ğŸ”· M0' if mkt == 0 else 'ğŸŸ¢ MR'\n",
    "    print(f'  {tag} | Market {mkt} | Rows: {len(d):4d} | RMSE: {root_mean_squared_error(d[\"actual\"], d[\"predicted\"]):.4f}')\n",
    "print('=' * 65)\n",
    "\n",
    "# â”€â”€ Top-10 Feature Importance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\n' + '=' * 65)\n",
    "print('ğŸ”· Top-10 Features â€” Model 0')\n",
    "print('=' * 65)\n",
    "imp_m0 = pd.Series(model_m0.feature_importance('gain'), index=feat_cols_m0)\n",
    "for f, v in imp_m0.nlargest(10).items():\n",
    "    print(f'    {f}: {v:.0f}')\n",
    "\n",
    "print('\\n' + '=' * 65)\n",
    "print('ğŸŸ¢ Top-10 Features â€” Model Rest')\n",
    "print('=' * 65)\n",
    "imp_rest = pd.Series(model_rest.feature_importance('gain'), index=feat_cols)\n",
    "for f, v in imp_rest.nlargest(10).items():\n",
    "    print(f'    {f}: {v:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae5bfd",
   "metadata": {},
   "source": [
    "## Final Retrain & Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd347c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GENERATE TEST PREDICTIONS (Time-Decay M0 + Static Rest)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Retrain on ALL observed data (train+val) for final submission\n",
    "print(\"Retraining on full observed data for submission...\")\n",
    "\n",
    "mask_all_m0   = X_all[\"market_id\"] == 0\n",
    "mask_all_rest = X_all[\"market_id\"] > 0\n",
    "\n",
    "# Model 0 final â€” time-decay weights on FULL observed Market 0 data\n",
    "X_all_m0 = X_all[mask_all_m0]\n",
    "y_all_m0 = y_all[mask_all_m0.values]\n",
    "weights_all_m0 = np.linspace(0.1, 1.0, len(X_all_m0))\n",
    "\n",
    "ds_all_m0 = lgb.Dataset(\n",
    "    X_all_m0[feat_cols_m0], y_all_m0,\n",
    "    weight=weights_all_m0,\n",
    "    categorical_feature=\"auto\", free_raw_data=False,\n",
    ")\n",
    "model_m0_final = lgb.train(\n",
    "    baseline_params_m0, ds_all_m0,\n",
    "    num_boost_round=int(model_m0.best_iteration * 1.1),\n",
    ")\n",
    "\n",
    "# Model Rest final â€” use STATIC baseline params (no weights)\n",
    "ds_all_rest = lgb.Dataset(\n",
    "    X_all[mask_all_rest], y_all[mask_all_rest.values],\n",
    "    categorical_feature=cat_idx if cat_idx else \"auto\", free_raw_data=False,\n",
    ")\n",
    "model_rest_final = lgb.train(\n",
    "    baseline_params, ds_all_rest,\n",
    "    num_boost_round=int(model_rest.best_iteration * 1.1),\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "mask_test_m0   = X_test[\"market_id\"] == 0\n",
    "mask_test_rest = X_test[\"market_id\"] > 0\n",
    "\n",
    "lgb_test_preds = np.empty(len(X_test))\n",
    "lgb_test_preds[mask_test_m0.values]   = np.sinh(model_m0_final.predict(X_test[mask_test_m0][feat_cols_m0]))\n",
    "lgb_test_preds[mask_test_rest.values] = np.sinh(model_rest_final.predict(X_test[mask_test_rest]))\n",
    "\n",
    "print(f\"âœ… Test predictions generated: {len(lgb_test_preds):,} rows\")\n",
    "print(f\"   Market 0 test rows:    {mask_test_m0.sum():,}  (time-decay weighted)\")\n",
    "print(f\"   Markets 1-5 test rows: {mask_test_rest.sum():,}  (static baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f680ead",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CREATE SUBMISSION FILE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Build prediction DataFrame keyed by id (our test_df is sorted by market/time,\n",
    "# but sample_submission has a different id order â€” we must match it exactly)\n",
    "pred_df = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"].values,\n",
    "    \"target\": lgb_test_preds\n",
    "})\n",
    "\n",
    "# Merge onto sample_sub to guarantee identical id order\n",
    "submission = sample_sub[[\"id\"]].merge(pred_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# Sanity checks\n",
    "assert len(submission) == len(sample_sub), f\"Row mismatch: {len(submission)} vs {len(sample_sub)}\"\n",
    "assert submission[\"target\"].isna().sum() == 0, f\"Missing predictions for {submission['target'].isna().sum()} ids!\"\n",
    "assert (submission[\"id\"] == sample_sub[\"id\"]).all(), \"ID order mismatch!\"\n",
    "\n",
    "# Save\n",
    "submission_path = \"../submissions/submission_split_timedecay.csv\"\n",
    "import os\n",
    "os.makedirs(\"../submissions\", exist_ok=True)\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission saved: {submission_path}\")\n",
    "print(f\"   Rows: {len(submission):,}\")\n",
    "print(f\"   ID order matches sample_submission: âœ“\")\n",
    "print(f\"   Target stats:\")\n",
    "print(f\"     Mean:   {submission['target'].mean():.4f}\")\n",
    "print(f\"     Std:    {submission['target'].std():.4f}\")\n",
    "print(f\"     Min:    {submission['target'].min():.4f}\")\n",
    "print(f\"     Max:    {submission['target'].max():.4f}\")\n",
    "print(f\"     Median: {submission['target'].median():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitor_kuas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}