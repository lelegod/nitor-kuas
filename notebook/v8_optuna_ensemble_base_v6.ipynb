{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b202e7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26df276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import optuna\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.optimize import minimize as sp_minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa8a7d",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97cb5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test_for_participants.csv'\n",
    "sample_path = '../data/sample_submission.csv'\n",
    "\n",
    "# â”€â”€ Dual Validation Date Boundaries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Val Set 1 (Physics): Autumn/Winter 2024 â€” cold, dark, windy grid\n",
    "VAL_PHYSICS_START = '2024-09-01'\n",
    "VAL_PHYSICS_END   = '2025-01-01'\n",
    "# Val Set 2 (Recency): Summer 2025 â€” last 3 months before LB\n",
    "VAL_RECENCY_START = '2025-06-01'\n",
    "\n",
    "SEED = 42\n",
    "N_TRIALS_LGB = 30\n",
    "SAVED_LGB_PATH = '../models/lgb_final.txt'\n",
    "ROUND_MULTIPLIER = 1.15\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd00b1c",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f7e012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(train_path)\n",
    "test_raw = pd.read_csv(test_path)\n",
    "sample_sub = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f72d6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>market</th>\n",
       "      <th>global_horizontal_irradiance</th>\n",
       "      <th>diffuse_horizontal_irradiance</th>\n",
       "      <th>direct_normal_irradiance</th>\n",
       "      <th>cloud_cover_total</th>\n",
       "      <th>cloud_cover_low</th>\n",
       "      <th>cloud_cover_mid</th>\n",
       "      <th>cloud_cover_high</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_80m</th>\n",
       "      <th>wind_direction_80m</th>\n",
       "      <th>wind_gust_speed_10m</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>solar_forecast</th>\n",
       "      <th>wind_forecast</th>\n",
       "      <th>load_forecast</th>\n",
       "      <th>delivery_start</th>\n",
       "      <th>delivery_end</th>\n",
       "      <th>is_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.913</td>\n",
       "      <td>Market A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.253719</td>\n",
       "      <td>245.501450</td>\n",
       "      <td>25.199999</td>\n",
       "      <td>15.077082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24050.1</td>\n",
       "      <td>38163.0100</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>Market A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.918108</td>\n",
       "      <td>242.241547</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>14.186923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23886.3</td>\n",
       "      <td>37379.1898</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.107</td>\n",
       "      <td>Market A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.983196</td>\n",
       "      <td>224.999893</td>\n",
       "      <td>21.240000</td>\n",
       "      <td>12.413477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23366.5</td>\n",
       "      <td>36336.8303</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.035</td>\n",
       "      <td>Market A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.218153</td>\n",
       "      <td>229.600174</td>\n",
       "      <td>16.199999</td>\n",
       "      <td>10.483357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22829.8</td>\n",
       "      <td>35337.7595</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>Market A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.210381</td>\n",
       "      <td>244.113022</td>\n",
       "      <td>18.359999</td>\n",
       "      <td>11.918120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22347.6</td>\n",
       "      <td>34474.3403</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2023-01-01 05:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target    market  global_horizontal_irradiance  \\\n",
       "0   0  -1.913  Market A                           0.0   \n",
       "1   5  -0.839  Market A                           0.0   \n",
       "2  10  -1.107  Market A                           0.0   \n",
       "3  15   0.035  Market A                           0.0   \n",
       "4  20  -0.829  Market A                           0.0   \n",
       "\n",
       "   diffuse_horizontal_irradiance  direct_normal_irradiance  cloud_cover_total  \\\n",
       "0                            0.0                       0.0                2.0   \n",
       "1                            0.0                       0.0               15.0   \n",
       "2                            0.0                       0.0               17.0   \n",
       "3                            0.0                       0.0               16.0   \n",
       "4                            0.0                       0.0               10.0   \n",
       "\n",
       "   cloud_cover_low  cloud_cover_mid  cloud_cover_high  ...  wind_speed_80m  \\\n",
       "0              0.0              0.0               2.0  ...       31.253719   \n",
       "1              0.0              0.0              15.0  ...       30.918108   \n",
       "2              0.0              0.0              17.0  ...       26.983196   \n",
       "3              0.0              0.0              16.0  ...       22.218153   \n",
       "4              0.0              0.0              10.0  ...       27.210381   \n",
       "\n",
       "   wind_direction_80m  wind_gust_speed_10m  wind_speed_10m  solar_forecast  \\\n",
       "0          245.501450            25.199999       15.077082             0.0   \n",
       "1          242.241547            23.400000       14.186923             0.0   \n",
       "2          224.999893            21.240000       12.413477             0.0   \n",
       "3          229.600174            16.199999       10.483357             0.0   \n",
       "4          244.113022            18.359999       11.918120             0.0   \n",
       "\n",
       "   wind_forecast  load_forecast      delivery_start        delivery_end  \\\n",
       "0        24050.1     38163.0100 2023-01-01 00:00:00 2023-01-01 01:00:00   \n",
       "1        23886.3     37379.1898 2023-01-01 01:00:00 2023-01-01 02:00:00   \n",
       "2        23366.5     36336.8303 2023-01-01 02:00:00 2023-01-01 03:00:00   \n",
       "3        22829.8     35337.7595 2023-01-01 03:00:00 2023-01-01 04:00:00   \n",
       "4        22347.6     34474.3403 2023-01-01 04:00:00 2023-01-01 05:00:00   \n",
       "\n",
       "   is_test  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in [train_raw, test_raw]:\n",
    "    df['delivery_start'] = pd.to_datetime(df['delivery_start'])\n",
    "    df['delivery_end'] = pd.to_datetime(df['delivery_end'])\n",
    "\n",
    "train_raw['is_test'] = 0\n",
    "test_raw['is_test'] = 1\n",
    "test_raw['target'] = np.nan\n",
    "\n",
    "df = pd.concat([train_raw, test_raw], ignore_index=True)\n",
    "df = df.sort_values(['market', 'delivery_start']).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f9f7757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-251.794)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2c5d5",
   "metadata": {},
   "source": [
    "## Enhanced Feature Engineering - Ultra Advanced Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83988d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic time features\n",
    "ds = df[\"delivery_start\"]\n",
    "df[\"hour\"]         = ds.dt.hour\n",
    "df[\"day_of_week\"]  = ds.dt.dayofweek\n",
    "df[\"day_of_month\"] = ds.dt.day\n",
    "df[\"month\"]        = ds.dt.month\n",
    "df[\"quarter\"]      = ds.dt.quarter\n",
    "df[\"day_of_year\"]  = ds.dt.dayofyear\n",
    "df[\"year\"]         = ds.dt.year\n",
    "df[\"is_weekend\"]   = (ds.dt.dayofweek >= 5).astype(np.int8)\n",
    "df[\"week_of_year\"] = ds.dt.isocalendar().week.astype(int)\n",
    "\n",
    "# Cyclical encoding for time features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Market encoding\n",
    "market_map = {f\"Market {c}\": i for i, c in enumerate(\"ABCDEF\")}\n",
    "df[\"market_id\"] = df[\"market\"].map(market_map).astype(np.int8)\n",
    "\n",
    "# Advanced demand and supply features\n",
    "df[\"residual_demand\"] = df[\"load_forecast\"] - df[\"solar_forecast\"] - df[\"wind_forecast\"]\n",
    "df[\"supply_ratio\"] = (df[\"solar_forecast\"] + df[\"wind_forecast\"]) / (df[\"load_forecast\"] + 1)\n",
    "df[\"renewable_ratio\"] = (df[\"solar_forecast\"] + df[\"wind_forecast\"]) / (df[\"solar_forecast\"] + df[\"wind_forecast\"] + df[\"load_forecast\"] + 1)\n",
    "df[\"net_supply\"] = df[\"solar_forecast\"] + df[\"wind_forecast\"]\n",
    "df[\"demand_supply_balance\"] = df[\"load_forecast\"] / (df[\"solar_forecast\"] + df[\"wind_forecast\"] + 1)\n",
    "\n",
    "# Tightness ratios\n",
    "df[\"tightness_ratio\"] = df[\"residual_demand\"] / (df[\"load_forecast\"] + 1)\n",
    "df[\"tightness_x_month\"] = df[\"tightness_ratio\"] * df[\"month\"]\n",
    "df[\"tightness_x_hour\"] = df[\"tightness_ratio\"] * df[\"hour\"]\n",
    "df[\"tightness_x_dow\"] = df[\"tightness_ratio\"] * df[\"day_of_week\"]\n",
    "\n",
    "# Price sensitivity indicators\n",
    "df[\"solar_wind_ratio\"] = df[\"solar_forecast\"] / (df[\"wind_forecast\"] + 1)\n",
    "df[\"wind_solar_ratio\"] = df[\"wind_forecast\"] / (df[\"solar_forecast\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "advanced_weather_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Advanced Weather Physics Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# First, alias columns whose names differ between dataset and our code\n",
    "\n",
    "if 'convective_available_potential_energy' in df.columns:\n",
    "    df['cape'] = df['convective_available_potential_energy']\n",
    "if 'precipitation_amount' in df.columns:\n",
    "    df['precipitation'] = df['precipitation_amount']\n",
    "if 'apparent_temperature_2m' in df.columns:\n",
    "    df['apparent_temperature'] = df['apparent_temperature_2m']\n",
    "if 'freezing_level_height' in df.columns:\n",
    "    df['boundary_layer_height'] = df['freezing_level_height']\n",
    "\n",
    "# Estimate missing columns from available physics\n",
    "# Saturation vapour pressure (Tetens formula)\n",
    "es = 6.112 * np.exp((17.67 * df['air_temperature_2m']) / (df['air_temperature_2m'] + 243.5))\n",
    "ea = (df['relative_humidity_2m'] / 100.0) * es\n",
    "df['vapour_pressure_deficit_2m'] = es - ea\n",
    "\n",
    "# Proxy precipitation probability from relative humidity\n",
    "df['precipitation_probability'] = np.where(df['relative_humidity_2m'] > 85, 50, 0)\n",
    "\n",
    "# â”€â”€ Derived weather features â”€â”€\n",
    "df['dew_point_depression']  = df['air_temperature_2m'] - df['dew_point_temperature_2m']\n",
    "df['wet_bulb_depression']   = df['air_temperature_2m'] - df['wet_bulb_temperature_2m']\n",
    "df['humidity_ratio']        = (0.622 * df['vapour_pressure_deficit_2m']) / (df['surface_pressure'] - df['vapour_pressure_deficit_2m'])\n",
    "df['blh_normalized_pressure'] = df['boundary_layer_height'] / (df['surface_pressure'] / 1000)\n",
    "\n",
    "# Wind shear (10m vs 80m)\n",
    "df['wind_shear']       = df['wind_speed_80m'] - df['wind_speed_10m']\n",
    "df['wind_shear_ratio'] = df['wind_speed_80m'] / (df['wind_speed_10m'] + 0.1)\n",
    "\n",
    "# Convection indices\n",
    "df['cape_cin_interaction'] = df['cape'] * df['convective_inhibition']\n",
    "df['convection_potential'] = df['cape'] / (abs(df['convective_inhibition']) + 1)\n",
    "\n",
    "# Visibility & cloud\n",
    "df['visibility_cloud_interaction'] = df['visibility'] / (df['cloud_cover_total'] + 1)\n",
    "\n",
    "# Combined weather severity index\n",
    "df['weather_severity'] = (\n",
    "    df['cloud_cover_total'] / 100 +\n",
    "    (100 - df['visibility'].clip(0, 100)) / 100 +\n",
    "    df['precipitation_probability'] / 100 +\n",
    "    df['cape'] / 1000\n",
    ") / 4\n",
    "\n",
    "# Solar / wind potential\n",
    "df['solar_potential'] = df['global_horizontal_irradiance'] * (1 - df['cloud_cover_total'] / 100)\n",
    "df['wind_potential']  = df['wind_speed_80m'] ** 3  # cubic âˆ power\n",
    "\n",
    "# Extreme weather flags\n",
    "df['extreme_temp']   = ((df['air_temperature_2m'] > 30) | (df['air_temperature_2m'] < -5)).astype(int)\n",
    "df['extreme_wind']   = (df['wind_speed_80m'] > 25).astype(int)\n",
    "df['extreme_precip'] = (df['precipitation'] > 5).astype(int)\n",
    "\n",
    "# Seasonal Ã— weather interactions\n",
    "df['temp_month_interaction'] = df['air_temperature_2m'] * df['month']\n",
    "df['wind_month_interaction'] = df['wind_speed_80m'] * df['month']\n",
    "df['solar_hour_interaction'] = df['solar_forecast'] * df['hour']\n",
    "\n",
    "# Heating / cooling degree-hours\n",
    "df['cooling_degree_hours'] = np.maximum(df['air_temperature_2m'] - 22, 0)\n",
    "df['heating_degree_hours'] = np.maximum(18 - df['air_temperature_2m'], 0)\n",
    "\n",
    "# Vapour pressure deficit normalised\n",
    "df['vpd_normalized'] = df['vapour_pressure_deficit_2m'] / df['surface_pressure']\n",
    "\n",
    "# Apparent temperature anomaly\n",
    "df['apparent_temp_anomaly']   = df['apparent_temperature'] - df['air_temperature_2m']\n",
    "df['apparent_air_temp_ratio'] = df['apparent_temperature'] / (df['air_temperature_2m'] + 1)\n",
    "\n",
    "# Lifted Index feature (strong storm predictor)\n",
    "if 'lifted_index' in df.columns:\n",
    "    df['lifted_index_negative'] = (-df['lifted_index']).clip(lower=0)  # only instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "momentum_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Weather Momentum & Lag Features (NO target leakage) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "weather_cols = ['wind_speed_80m', 'solar_forecast', 'load_forecast',\n",
    "                'wind_forecast', 'air_temperature_2m']\n",
    "\n",
    "for col in weather_cols:\n",
    "    grp = df.groupby('market_id')[col]\n",
    "\n",
    "    # Hourly differences (momentum)\n",
    "    df[f'{col}_diff_1h']  = grp.diff(1)\n",
    "    df[f'{col}_diff_3h']  = grp.diff(3)\n",
    "    df[f'{col}_diff_6h']  = grp.diff(6)\n",
    "    df[f'{col}_diff_12h'] = grp.diff(12)\n",
    "\n",
    "    # Rolling mean / std\n",
    "    df[f'{col}_rolling_mean_6h']  = grp.transform(lambda x: x.rolling(6,  min_periods=1).mean())\n",
    "    df[f'{col}_rolling_std_6h']   = grp.transform(lambda x: x.rolling(6,  min_periods=1).std().fillna(0))\n",
    "    df[f'{col}_rolling_mean_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).mean())\n",
    "    df[f'{col}_rolling_std_24h']  = grp.transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "\n",
    "    # Rolling min / max (use bfill() instead of deprecated fillna(method=...))\n",
    "    df[f'{col}_rolling_min_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).min().bfill())\n",
    "    df[f'{col}_rolling_max_24h'] = grp.transform(lambda x: x.rolling(24, min_periods=1).max().bfill())\n",
    "    df[f'{col}_range_24h']       = df[f'{col}_rolling_max_24h'] - df[f'{col}_rolling_min_24h']\n",
    "\n",
    "    # Exponential weighted moving averages\n",
    "    df[f'{col}_ewm_6h']  = grp.transform(lambda x: x.ewm(span=6,  adjust=False).mean())\n",
    "    df[f'{col}_ewm_24h'] = grp.transform(lambda x: x.ewm(span=24, adjust=False).mean())\n",
    "\n",
    "    # Z-score vs rolling window\n",
    "    df[f'{col}_zscore_24h'] = (df[col] - df[f'{col}_rolling_mean_24h']) / (df[f'{col}_rolling_std_24h'] + 0.001)\n",
    "\n",
    "# Temperature anomaly vs recent history\n",
    "df['temp_24h_mean']    = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(24, min_periods=1).mean())\n",
    "df['temp_72h_mean']    = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(72, min_periods=1).mean())\n",
    "df['temp_anomaly_24h'] = df['air_temperature_2m'] - df['temp_24h_mean']\n",
    "df['temp_anomaly_72h'] = df['air_temperature_2m'] - df['temp_72h_mean']\n",
    "\n",
    "# Wind direction components & stability\n",
    "df['wind_dir_sin']    = np.sin(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_dir_cos']    = np.cos(np.deg2rad(df['wind_direction_80m']))\n",
    "df['wind_dir_change'] = df.groupby('market_id')['wind_direction_80m'].diff(1).abs()\n",
    "\n",
    "# Pressure & humidity interactions with temperature\n",
    "df['pressure_temp_interaction']  = df['surface_pressure'] * df['air_temperature_2m']\n",
    "df['humidity_temp_interaction']  = df['relative_humidity_2m'] * df['air_temperature_2m']\n",
    "df['pressure_gradient']          = df.groupby('market_id')['surface_pressure'].diff(1)\n",
    "\n",
    "# Cloud & precipitation transforms\n",
    "df['cloud_cover_total_sq']  = df['cloud_cover_total'] ** 2\n",
    "df['cloud_cover_effect']    = df['cloud_cover_total'] * df['global_horizontal_irradiance']\n",
    "df['precip_prob_sq']        = df['precipitation_probability'] ** 2\n",
    "df['precip_effect']         = df['precipitation'] * df['precipitation_probability']\n",
    "\n",
    "# Radiation efficiency\n",
    "df['solar_efficiency']       = df['solar_forecast'] / (df['global_horizontal_irradiance'] + 1)\n",
    "df['radiation_cloud_ratio']  = df['global_horizontal_irradiance'] / (df['cloud_cover_total'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "temporal_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced temporal features\n",
    "\n",
    "# Hourly and daily patterns\n",
    "df['hour_from_peak'] = abs(df['hour'] - 12)  # Distance from peak solar hour\n",
    "df['is_peak_solar'] = ((df['hour'] >= 10) & (df['hour'] <= 16)).astype(int)\n",
    "df['is_off_peak'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "df['is_business_hours'] = ((df['hour'] >= 8) & (df['hour'] <= 18) & (df['day_of_week'] < 5)).astype(int)\n",
    "\n",
    "# Week patterns\n",
    "df['is_monday'] = (df['day_of_week'] == 0).astype(int)\n",
    "df['is_friday'] = (df['day_of_week'] == 4).astype(int)\n",
    "df['is_weekend_start'] = (df['day_of_week'] == 4).astype(int)  # Friday\n",
    "df['is_weekend_end'] = (df['day_of_week'] == 6).astype(int)    # Sunday\n",
    "\n",
    "# Monthly patterns\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "df['is_autumn'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "\n",
    "# Quarter interactions\n",
    "df['q1_temp_interaction'] = (df['quarter'] == 1) * df['air_temperature_2m']\n",
    "df['q2_temp_interaction'] = (df['quarter'] == 2) * df['air_temperature_2m']\n",
    "df['q3_temp_interaction'] = (df['quarter'] == 3) * df['air_temperature_2m']\n",
    "df['q4_temp_interaction'] = (df['quarter'] == 4) * df['air_temperature_2m']\n",
    "\n",
    "# Seasonal demand patterns\n",
    "df['winter_load_factor'] = df['is_winter'] * df['load_forecast']\n",
    "df['summer_load_factor'] = df['is_summer'] * df['load_forecast']\n",
    "df['spring_load_factor'] = df['is_spring'] * df['load_forecast']\n",
    "df['autumn_load_factor'] = df['is_autumn'] * df['load_forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "historical_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Safe historical encoding: 7 features\n",
      "   Global training mean: 37.3241\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Historical Target Encoding  (VALIDATION-SAFE MODE â€” v6 Dual Val)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# For tuning/validation: compute group means ONLY from TRAINING data\n",
    "# (excluding both validation sets) to avoid any leakage.\n",
    "\n",
    "# âš ï¸ VALIDATION MODE: using only training data (pre-Physics val start)\n",
    "# For final submission, change to:\n",
    "#     strict_train = df[(df['is_test'] == 0) & (df['delivery_start'] < VAL_PHYSICS_START)]\n",
    "strict_train = df[(df['is_test'] == 0) & (df['delivery_start'] < VAL_PHYSICS_START)]\n",
    "\n",
    "# â”€â”€ Market Ã— Hour mean â”€â”€\n",
    "mean_mh = (strict_train\n",
    "    .groupby(['market_id', 'hour'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mh'))\n",
    "\n",
    "# â”€â”€ Market Ã— Day-of-week mean â”€â”€\n",
    "mean_mdow = (strict_train\n",
    "    .groupby(['market_id', 'day_of_week'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mdow'))\n",
    "\n",
    "# â”€â”€ Market Ã— Month mean â”€â”€\n",
    "mean_mm = (strict_train\n",
    "    .groupby(['market_id', 'month'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mm'))\n",
    "\n",
    "# â”€â”€ Market mean (global baseline per market) â”€â”€\n",
    "mean_m = (strict_train\n",
    "    .groupby(['market_id'])['target']\n",
    "    .mean().reset_index(name='target_histmean_m'))\n",
    "\n",
    "# â”€â”€ Hour mean (global baseline per hour) â”€â”€\n",
    "mean_h = (strict_train\n",
    "    .groupby(['hour'])['target']\n",
    "    .mean().reset_index(name='target_histmean_h'))\n",
    "\n",
    "# â”€â”€ Market Ã— Hour Ã— DayOfWeek mean â”€â”€\n",
    "mean_mhd = (strict_train\n",
    "    .groupby(['market_id', 'hour', 'day_of_week'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mhd'))\n",
    "\n",
    "# â”€â”€ Market Ã— Quarter mean â”€â”€\n",
    "mean_mq = (strict_train\n",
    "    .groupby(['market_id', 'quarter'])['target']\n",
    "    .mean().reset_index(name='target_histmean_mq'))\n",
    "\n",
    "# Merge ALL onto full dataframe\n",
    "df = df.merge(mean_mh,  on=['market_id', 'hour'],          how='left')\n",
    "df = df.merge(mean_mdow, on=['market_id', 'day_of_week'],  how='left')\n",
    "df = df.merge(mean_mm,  on=['market_id', 'month'],          how='left')\n",
    "df = df.merge(mean_m,   on=['market_id'],                   how='left')\n",
    "df = df.merge(mean_h,   on=['hour'],                        how='left')\n",
    "df = df.merge(mean_mhd, on=['market_id', 'hour', 'day_of_week'], how='left')\n",
    "df = df.merge(mean_mq,  on=['market_id', 'quarter'],       how='left')\n",
    "\n",
    "# Fill any NaN hist-means with global training mean\n",
    "global_mean = strict_train['target'].mean()\n",
    "for c in [c for c in df.columns if c.startswith('target_histmean_')]:\n",
    "    df[c] = df[c].fillna(global_mean)\n",
    "\n",
    "# â”€â”€ Deviations from historical baselines (computed from safe features only) â”€â”€\n",
    "df['histmean_mh_x_residual'] = df['target_histmean_mh'] * df['residual_demand']\n",
    "df['histmean_mh_x_tightness'] = df['target_histmean_mh'] * df['tightness_ratio']\n",
    "df['histmean_deviation_dow_vs_m'] = df['target_histmean_mdow'] - df['target_histmean_m']\n",
    "df['histmean_deviation_mh_vs_h'] = df['target_histmean_mh'] - df['target_histmean_h']\n",
    "\n",
    "print(f\"âœ… Safe historical encoding: {sum(c.startswith('target_histmean') for c in df.columns)} features\")\n",
    "print(f\"   Global training mean: {global_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "interaction_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature engineering complete: 235 total columns\n",
      "   Training rows: 132608, Test rows: 13098\n",
      "   NaN check (target): 13098 NaN (should equal test rows)\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Interaction features (weather Ã— demand, cross-market) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['temp_load_interaction']  = df['air_temperature_2m'] * df['load_forecast']\n",
    "df['wind_load_interaction']  = df['wind_speed_80m'] * df['load_forecast']\n",
    "df['solar_load_interaction'] = df['solar_forecast'] * df['load_forecast']\n",
    "df['temp_wind_interaction']  = df['air_temperature_2m'] * df['wind_speed_80m']\n",
    "df['temp_solar_interaction'] = df['air_temperature_2m'] * df['solar_forecast']\n",
    "df['wind_solar_interaction'] = df['wind_speed_80m'] * df['solar_forecast']\n",
    "\n",
    "# Triple interactions\n",
    "df['temp_wind_load_interaction']  = df['air_temperature_2m'] * df['wind_speed_80m'] * df['load_forecast']\n",
    "df['temp_solar_load_interaction'] = df['air_temperature_2m'] * df['solar_forecast'] * df['load_forecast']\n",
    "df['wind_solar_load_interaction'] = df['wind_speed_80m'] * df['solar_forecast'] * df['load_forecast']\n",
    "\n",
    "# Weather volatility (rolling on weather â€” safe, no target)\n",
    "df['temp_volatility']  = df.groupby('market_id')['air_temperature_2m'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "df['wind_volatility']  = df.groupby('market_id')['wind_speed_80m'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "df['solar_volatility'] = df.groupby('market_id')['solar_forecast'].transform(lambda x: x.rolling(24, min_periods=1).std().fillna(0))\n",
    "\n",
    "# Rate of change (weather â€” safe)\n",
    "df['temp_rate_change']  = df.groupby('market_id')['air_temperature_2m'].diff(1) / (df.groupby('market_id')['air_temperature_2m'].shift(1).abs() + 0.01)\n",
    "df['wind_rate_change']  = df.groupby('market_id')['wind_speed_80m'].diff(1) / (df.groupby('market_id')['wind_speed_80m'].shift(1).abs() + 0.01)\n",
    "df['solar_rate_change'] = df.groupby('market_id')['solar_forecast'].diff(1) / (df.groupby('market_id')['solar_forecast'].shift(1).abs() + 0.01)\n",
    "\n",
    "# Cross-market features (same timestamp, across markets â€” safe)\n",
    "for col in ['wind_speed_80m', 'solar_forecast', 'load_forecast']:\n",
    "    ts_mean = df.groupby('delivery_start')[col].transform('mean')\n",
    "    ts_std  = df.groupby('delivery_start')[col].transform('std') + 0.001\n",
    "    df[f'{col}_market_diff']   = df[col] - ts_mean\n",
    "    df[f'{col}_market_zscore'] = (df[col] - ts_mean) / ts_std\n",
    "\n",
    "# Advanced rolling statistics (weather â€” safe)\n",
    "for col in ['wind_speed_80m', 'solar_forecast', 'load_forecast']:\n",
    "    df[f'{col}_skew_24h'] = df.groupby('market_id')[col].transform(\n",
    "        lambda x: x.rolling(24, min_periods=12).skew().fillna(0))\n",
    "\n",
    "\n",
    "# â”€â”€ SUMMER HEATWAVE & GRID STRESS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Heatwave Penalty (Exponential stress when Temp > 25C and Wind is low)\n",
    "df['heatwave_stress'] = np.where(\n",
    "    (df['air_temperature_2m'] > 25) & (df['wind_speed_80m'] < 5),\n",
    "    (df['air_temperature_2m'] - 25) ** 2, \n",
    "    0\n",
    ")\n",
    "\n",
    "# 2. Solar/Wind Drought Flag\n",
    "df['renewable_drought'] = ((df['solar_forecast'] < 10) & (df['wind_forecast'] < 10)).astype(int)\n",
    "\n",
    "# 3. Summer Cooling Load Proxy\n",
    "df['cooling_degree_load'] = df['load_forecast'] * np.maximum(0, df['air_temperature_2m'] - 22)\n",
    "\n",
    "# â”€â”€ Final NaN handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NEVER fill target NaN â€” those are test rows!\n",
    "exclude_from_fill = {'target', 'delivery_start', 'delivery_end', 'market', 'id'}\n",
    "for col in df.columns:\n",
    "    if col in exclude_from_fill:\n",
    "        continue\n",
    "    if df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int8']:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(f\"âœ… Feature engineering complete: {len(df.columns)} total columns\")\n",
    "print(f\"   Training rows: {(df['is_test']==0).sum()}, Test rows: {(df['is_test']==1).sum()}\")\n",
    "print(f\"   NaN check (target): {df['target'].isna().sum()} NaN (should equal test rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8cf65",
   "metadata": {},
   "source": [
    "## Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adc058ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Training rows: 101,792\n",
      "â„ï¸ Val 1 (Physics â€” Autumn/Winter 2024): 17,568\n",
      "ðŸ”¥ Val 2 (Recency â€” Summer 2025):        13,248\n"
     ]
    }
   ],
   "source": [
    "observed_df = df[df['is_test'] == 0].copy()\n",
    "test_df = df[df['is_test'] == 1].copy()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DUAL VALIDATION SPLITTING LOGIC\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. Val Set 1: The \"Physics\" Test (Autumn/Winter 2024)\n",
    "mask_val_physics = (observed_df['delivery_start'] >= VAL_PHYSICS_START) & (observed_df['delivery_start'] < VAL_PHYSICS_END)\n",
    "\n",
    "# 2. Val Set 2: The \"Recency\" Test (Summer 2025 â€” last 3 months)\n",
    "mask_val_recency = (observed_df['delivery_start'] >= VAL_RECENCY_START)\n",
    "\n",
    "# 3. Train Set: Everything else\n",
    "mask_train = ~(mask_val_physics | mask_val_recency)\n",
    "\n",
    "train_df = observed_df[mask_train]\n",
    "val_physics_df = observed_df[mask_val_physics]\n",
    "val_recency_df = observed_df[mask_val_recency]\n",
    "\n",
    "print(f'ðŸŒ² Training rows: {len(train_df):,}')\n",
    "print(f'â„ï¸ Val 1 (Physics â€” Autumn/Winter 2024): {len(val_physics_df):,}')\n",
    "print(f'ðŸ”¥ Val 2 (Recency â€” Summer 2025):        {len(val_recency_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "844edcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = set(['id', 'target', 'market', 'delivery_start', 'delivery_end', 'is_test'])\n",
    "feat_cols = sorted([c for c in df.columns if c not in drop_cols])\n",
    "cat_idx = [feat_cols.index('market_id')] if 'market_id' in feat_cols else []\n",
    "\n",
    "X_train = train_df[feat_cols]\n",
    "y_train_real = train_df['target'].values\n",
    "y_train = np.arcsinh(train_df['target'].values)\n",
    "\n",
    "X_val_physics = val_physics_df[feat_cols]\n",
    "y_val_physics_real = val_physics_df['target'].values\n",
    "y_val_physics = np.arcsinh(val_physics_df['target'].values)\n",
    "\n",
    "X_val_recency = val_recency_df[feat_cols]\n",
    "y_val_recency_real = val_recency_df['target'].values\n",
    "y_val_recency = np.arcsinh(val_recency_df['target'].values)\n",
    "\n",
    "X_all = observed_df[feat_cols]\n",
    "y_all_real = observed_df['target'].values\n",
    "y_all = np.arcsinh(observed_df['target'].values)\n",
    "\n",
    "X_test = test_df[feat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccae2d",
   "metadata": {},
   "source": [
    "## PREPARATION & M0 NUCLEAR DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77ce45d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0  Train (nuclear): 9,478\n",
      "Rest Train:          83,555\n",
      "M0  Val Physics:     2,928\n",
      "Rest Val Physics:    14,640\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import numpy as np, pandas as pd\n",
    "import optuna, xgboost as xgb, lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.optimize import minimize as sp_minimize\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "os.makedirs('../models/v8/', exist_ok=True)\n",
    "os.makedirs('../submissions/', exist_ok=True)\n",
    "\n",
    "# â”€â”€ Market masks on TRAINING data â”€â”€\n",
    "mask_tr_m0   = X_train['market_id'] == 0\n",
    "mask_tr_rest = X_train['market_id'] > 0\n",
    "\n",
    "# Nuclear Drop: M0 training only from 2024+\n",
    "nuclear = train_df['delivery_start'] >= '2024-01-01'\n",
    "X_train_m0   = X_train[mask_tr_m0 & nuclear].drop(columns=['market_id'])\n",
    "y_train_m0   = y_train[(mask_tr_m0 & nuclear).values]\n",
    "X_train_rest = X_train[mask_tr_rest]\n",
    "y_train_rest = y_train[mask_tr_rest.values]\n",
    "\n",
    "# â”€â”€ Market masks on PHYSICS VAL data â”€â”€\n",
    "mask_vp_m0   = X_val_physics['market_id'] == 0\n",
    "mask_vp_rest = X_val_physics['market_id'] > 0\n",
    "X_vp_m0      = X_val_physics[mask_vp_m0].drop(columns=['market_id'])\n",
    "y_vp_m0      = y_val_physics[mask_vp_m0.values]\n",
    "y_vp_real_m0 = y_val_physics_real[mask_vp_m0.values]\n",
    "X_vp_rest      = X_val_physics[mask_vp_rest]\n",
    "y_vp_rest      = y_val_physics[mask_vp_rest.values]\n",
    "y_vp_real_rest = y_val_physics_real[mask_vp_rest.values]\n",
    "\n",
    "# Feature list without market_id (for M0 models)\n",
    "feat_m0 = [c for c in feat_cols if c != 'market_id']\n",
    "\n",
    "print(f\"M0  Train (nuclear): {len(X_train_m0):,}\")\n",
    "print(f\"Rest Train:          {len(X_train_rest):,}\")\n",
    "print(f\"M0  Val Physics:     {len(X_vp_m0):,}\")\n",
    "print(f\"Rest Val Physics:    {len(X_vp_rest):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6a312",
   "metadata": {},
   "source": [
    "## BLOCK A â€” LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73edd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK A: LightGBM\n",
      "============================================================\n",
      "  LGB M0 â€¦\n",
      "  Best M0  RMSE=80.1025\n",
      "  LGB Rest â€¦\n",
      "  Best Rest RMSE=39.2627\n",
      "  LGB done in 2122s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(\"=\"*60); print(\"BLOCK A: LightGBM\"); print(\"=\"*60)\n",
    "\n",
    "def _lgb_obj(trial, Xtr, ytr, Xvl, yvl, yvl_real, cat_idx=None):\n",
    "    p = {'objective':'huber','metric':'rmse','verbosity':-1,'seed':SEED,'n_jobs':-1,\n",
    "         'learning_rate': trial.suggest_float('lr', 1e-3, 0.1, log=True),\n",
    "         'num_leaves':    trial.suggest_int('nl', 31, 512),\n",
    "         'alpha':         trial.suggest_float('alpha', 1.0, 3.0)}\n",
    "    ds_t = lgb.Dataset(Xtr, ytr, categorical_feature=cat_idx or 'auto', free_raw_data=False)\n",
    "    ds_v = lgb.Dataset(Xvl, yvl, reference=ds_t, free_raw_data=False)\n",
    "    m = lgb.train(p, ds_t, 2000, valid_sets=[ds_v],\n",
    "                  callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    trial.set_user_attr('bi', m.best_iteration)\n",
    "    return root_mean_squared_error(yvl_real, np.sinh(m.predict(Xvl)))\n",
    "\n",
    "print(\"  LGB M0 â€¦\")\n",
    "s_lgb_m0 = optuna.create_study(direction='minimize')\n",
    "s_lgb_m0.optimize(lambda t: _lgb_obj(t, X_train_m0, y_train_m0,\n",
    "                                       X_vp_m0, y_vp_m0, y_vp_real_m0), n_trials=25)\n",
    "print(f\"  Best M0  RMSE={s_lgb_m0.best_value:.4f}\")\n",
    "\n",
    "print(\"  LGB Rest â€¦\")\n",
    "cat_idx_rest = [list(X_train_rest.columns).index('market_id')]\n",
    "s_lgb_rest = optuna.create_study(direction='minimize')\n",
    "s_lgb_rest.optimize(lambda t: _lgb_obj(t, X_train_rest, y_train_rest,\n",
    "                                         X_vp_rest, y_vp_rest, y_vp_real_rest,\n",
    "                                         cat_idx_rest), n_trials=25)\n",
    "print(f\"  Best Rest RMSE={s_lgb_rest.best_value:.4f}\")\n",
    "\n",
    "# Train final val models with best params\n",
    "def _lgb_train(best, Xtr, ytr, Xvl, yvl, bi, cat_idx=None):\n",
    "    p = {'objective':'huber','metric':'rmse','verbosity':-1,'seed':SEED,'n_jobs':-1,\n",
    "         'learning_rate':best['lr'],'num_leaves':best['nl'],'alpha':best['alpha']}\n",
    "    ds_t = lgb.Dataset(Xtr, ytr, categorical_feature=cat_idx or 'auto')\n",
    "    ds_v = lgb.Dataset(Xvl, yvl, reference=ds_t)\n",
    "    return lgb.train(p, ds_t, bi, valid_sets=[ds_v],\n",
    "                     callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "\n",
    "bi_lgb_m0   = s_lgb_m0.best_trial.user_attrs['bi']\n",
    "bi_lgb_rest = s_lgb_rest.best_trial.user_attrs['bi']\n",
    "\n",
    "val_lgb_m0   = _lgb_train(s_lgb_m0.best_params,   X_train_m0, y_train_m0,\n",
    "                           X_vp_m0, y_vp_m0, bi_lgb_m0)\n",
    "val_lgb_rest = _lgb_train(s_lgb_rest.best_params, X_train_rest, y_train_rest,\n",
    "                           X_vp_rest, y_vp_rest, bi_lgb_rest, cat_idx_rest)\n",
    "\n",
    "val_lgb_m0.save_model('../models/v8/lgb_m0_val.txt')\n",
    "val_lgb_rest.save_model('../models/v8/lgb_rest_val.txt')\n",
    "\n",
    "json.dump({'m0':s_lgb_m0.best_params,'m0_bi':bi_lgb_m0,\n",
    "           'rest':s_lgb_rest.best_params,'rest_bi':bi_lgb_rest},\n",
    "          open('../models/v8/lgb_best_params.json','w'))\n",
    "print(f\"  LGB done in {time.time()-t0:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294e1c7",
   "metadata": {},
   "source": [
    "## BLOCK B â€” XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02468f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK B: XGBoost\n",
      "============================================================\n",
      "  XGB M0 â€¦\n",
      "  Best M0  RMSE=77.1926\n",
      "  XGB Rest â€¦\n",
      "  Best Rest RMSE=38.4633\n",
      "  XGB done.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60); print(\"BLOCK B: XGBoost\"); print(\"=\"*60)\n",
    "\n",
    "def _xgb_obj(trial, Xtr, ytr, Xvl, yvl, yvl_real):\n",
    "    # Fix: Changed 'reg:huber' to 'reg:pseudohubererror'\n",
    "    p = {\n",
    "        'tree_method': 'hist', \n",
    "        'objective': 'reg:pseudohubererror', # Corrected name\n",
    "        'huber_slope': trial.suggest_float('alpha', 1.0, 3.0), # 'alpha' in LGB is 'huber_slope' in XGB\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': SEED, \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('lr', 1e-3, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('md', 5, 12)\n",
    "    }\n",
    "    dt = xgb.DMatrix(Xtr, label=ytr); dv = xgb.DMatrix(Xvl, label=yvl)\n",
    "    m = xgb.train(p, dt, 2000, evals=[(dv,'v')],\n",
    "                  early_stopping_rounds=50, verbose_eval=False)\n",
    "    trial.set_user_attr('bi', m.best_iteration)\n",
    "    preds = np.clip(m.predict(dv), -20, 20)  # prevent sinh overflow\n",
    "    return root_mean_squared_error(yvl_real, np.sinh(preds))\n",
    "\n",
    "print(\"  XGB M0 â€¦\")\n",
    "s_xgb_m0 = optuna.create_study(direction='minimize')\n",
    "s_xgb_m0.optimize(lambda t: _xgb_obj(t, X_train_m0, y_train_m0,\n",
    "                                       X_vp_m0, y_vp_m0, y_vp_real_m0), n_trials=25)\n",
    "print(f\"  Best M0  RMSE={s_xgb_m0.best_value:.4f}\")\n",
    "\n",
    "print(\"  XGB Rest â€¦\")\n",
    "s_xgb_rest = optuna.create_study(direction='minimize')\n",
    "s_xgb_rest.optimize(lambda t: _xgb_obj(t, X_train_rest, y_train_rest,\n",
    "                                         X_vp_rest, y_vp_rest, y_vp_real_rest), n_trials=25)\n",
    "print(f\"  Best Rest RMSE={s_xgb_rest.best_value:.4f}\")\n",
    "\n",
    "# --- Retrain Best Models for Scipy Weights ---\n",
    "def _xgb_train(best, Xtr, ytr, Xvl, yvl, bi):\n",
    "    p = {\n",
    "        'tree_method': 'hist', \n",
    "        'objective': 'reg:pseudohubererror', # Corrected name\n",
    "        'huber_slope': best['alpha'],\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': SEED, \n",
    "        'n_jobs': -1, \n",
    "        'learning_rate': best['lr'], \n",
    "        'max_depth': best['md']\n",
    "    }\n",
    "    dt = xgb.DMatrix(Xtr, label=ytr); dv = xgb.DMatrix(Xvl, label=yvl)\n",
    "    return xgb.train(p, dt, bi, evals=[(dv,'v')],\n",
    "                     early_stopping_rounds=50, verbose_eval=False)\n",
    "\n",
    "bi_xgb_m0   = s_xgb_m0.best_trial.user_attrs['bi']\n",
    "bi_xgb_rest = s_xgb_rest.best_trial.user_attrs['bi']\n",
    "\n",
    "val_xgb_m0   = _xgb_train(s_xgb_m0.best_params,   X_train_m0, y_train_m0,\n",
    "                            X_vp_m0, y_vp_m0, bi_xgb_m0)\n",
    "val_xgb_rest = _xgb_train(s_xgb_rest.best_params, X_train_rest, y_train_rest,\n",
    "                            X_vp_rest, y_vp_rest, bi_xgb_rest)\n",
    "\n",
    "# Save validation models and params\n",
    "val_xgb_m0.save_model('../models/v8/xgb_m0_val.json')\n",
    "val_xgb_rest.save_model('../models/v8/xgb_rest_val.json')\n",
    "\n",
    "json.dump({'m0':s_xgb_m0.best_params,'m0_bi':bi_xgb_m0,\n",
    "           'rest':s_xgb_rest.best_params,'rest_bi':bi_xgb_rest},\n",
    "          open('../models/v8/xgb_best_params.json','w'))\n",
    "print(f\"  XGB done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640ed76",
   "metadata": {},
   "source": [
    "## BLOCK C â€” CatBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a306640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK C: CatBoost\n",
      "============================================================\n",
      "  CAT M0 â€¦\n",
      "  Best M0  RMSE=79.1533\n",
      "  CAT Rest â€¦\n",
      "  Best Rest RMSE=39.5610\n",
      "  CAT done in 1945s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(\"=\"*60); print(\"BLOCK C: CatBoost\"); print(\"=\"*60)\n",
    "\n",
    "def _cat_obj(trial, Xtr, ytr, Xvl, yvl, yvl_real, cat_feat=None):\n",
    "    p = {'loss_function':'Huber:delta=1.5','task_type':'CPU','eval_metric':'RMSE',\n",
    "         'random_seed':SEED,'verbose':False,'iterations':1000,'early_stopping_rounds':50,\n",
    "         'learning_rate': trial.suggest_float('lr', 1e-3, 0.1, log=True),\n",
    "         'depth':         trial.suggest_int('depth', 4, 10)}\n",
    "    m = CatBoostRegressor(**p)\n",
    "    if cat_feat:\n",
    "        m.fit(Pool(Xtr, ytr, cat_features=cat_feat),\n",
    "              eval_set=Pool(Xvl, yvl, cat_features=cat_feat), use_best_model=True)\n",
    "    else:\n",
    "        m.fit(Xtr, ytr, eval_set=(Xvl, yvl), use_best_model=True)\n",
    "    trial.set_user_attr('bi', m.get_best_iteration())\n",
    "    preds = np.clip(m.predict(Xvl), -20, 20)  # prevent sinh overflow\n",
    "    return root_mean_squared_error(yvl_real, np.sinh(preds))\n",
    "\n",
    "print(\"  CAT M0 â€¦\")\n",
    "s_cat_m0 = optuna.create_study(direction='minimize')\n",
    "s_cat_m0.optimize(lambda t: _cat_obj(t, X_train_m0, y_train_m0,\n",
    "                                      X_vp_m0, y_vp_m0, y_vp_real_m0), n_trials=15)\n",
    "print(f\"  Best M0  RMSE={s_cat_m0.best_value:.4f}\")\n",
    "\n",
    "print(\"  CAT Rest â€¦\")\n",
    "cat_feat_rest = [list(X_train_rest.columns).index('market_id')]\n",
    "s_cat_rest = optuna.create_study(direction='minimize')\n",
    "s_cat_rest.optimize(lambda t: _cat_obj(t, X_train_rest, y_train_rest,\n",
    "                                        X_vp_rest, y_vp_rest, y_vp_real_rest,\n",
    "                                        cat_feat_rest), n_trials=15)\n",
    "print(f\"  Best Rest RMSE={s_cat_rest.best_value:.4f}\")\n",
    "\n",
    "# Train final val models\n",
    "def _cat_train(best, Xtr, ytr, Xvl, yvl, bi, cat_feat=None):\n",
    "    p = {'loss_function':'Huber:delta=1.5','task_type':'CPU','eval_metric':'RMSE',\n",
    "         'random_seed':SEED,'verbose':False,'iterations':bi,\n",
    "         'learning_rate':best['lr'],'depth':best['depth']}\n",
    "    m = CatBoostRegressor(**p)\n",
    "    if cat_feat:\n",
    "        m.fit(Pool(Xtr, ytr, cat_features=cat_feat),\n",
    "              eval_set=Pool(Xvl, yvl, cat_features=cat_feat), use_best_model=True)\n",
    "    else:\n",
    "        m.fit(Xtr, ytr, eval_set=(Xvl, yvl), use_best_model=True)\n",
    "    return m\n",
    "\n",
    "bi_cat_m0   = s_cat_m0.best_trial.user_attrs['bi']\n",
    "bi_cat_rest = s_cat_rest.best_trial.user_attrs['bi']\n",
    "\n",
    "val_cat_m0   = _cat_train(s_cat_m0.best_params,   X_train_m0, y_train_m0,\n",
    "                           X_vp_m0, y_vp_m0, bi_cat_m0)\n",
    "val_cat_rest = _cat_train(s_cat_rest.best_params, X_train_rest, y_train_rest,\n",
    "                           X_vp_rest, y_vp_rest, bi_cat_rest, cat_feat_rest)\n",
    "\n",
    "val_cat_m0.save_model('../models/v8/cat_m0_val.cbm')\n",
    "val_cat_rest.save_model('../models/v8/cat_rest_val.cbm')\n",
    "\n",
    "json.dump({'m0':s_cat_m0.best_params,'m0_bi':bi_cat_m0,\n",
    "           'rest':s_cat_rest.best_params,'rest_bi':bi_cat_rest},\n",
    "          open('../models/v8/cat_best_params.json','w'))\n",
    "print(f\"  CAT done in {time.time()-t0:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c57ec",
   "metadata": {},
   "source": [
    "## BLOCK D â€” Scipy Blend-Weight Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8776bba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK D: SCIPY WEIGHT OPTIMIZATION\n",
      "============================================================\n",
      "  M0   weights [LGB,XGB,CAT]: [0. 1. 0.]  RMSE=77.1912\n",
      "  Rest weights [LGB,XGB,CAT]: [0.0852 0.9105 0.0042]  RMSE=38.4495\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60); print(\"BLOCK D: SCIPY WEIGHT OPTIMIZATION\"); print(\"=\"*60)\n",
    "\n",
    "# Val predictions from 6 validation-trained models\n",
    "vp_lgb_m0  = np.sinh(np.clip(val_lgb_m0.predict(X_vp_m0), -20, 20))\n",
    "vp_xgb_m0  = np.sinh(np.clip(val_xgb_m0.predict(xgb.DMatrix(X_vp_m0)), -20, 20))\n",
    "vp_cat_m0  = np.sinh(np.clip(val_cat_m0.predict(X_vp_m0), -20, 20))\n",
    "\n",
    "vp_lgb_r   = np.sinh(np.clip(val_lgb_rest.predict(X_vp_rest), -20, 20))\n",
    "vp_xgb_r   = np.sinh(np.clip(val_xgb_rest.predict(xgb.DMatrix(X_vp_rest)), -20, 20))\n",
    "vp_cat_r   = np.sinh(np.clip(val_cat_rest.predict(X_vp_rest), -20, 20))\n",
    "\n",
    "def _blend_rmse(w, p1, p2, p3, y_real):\n",
    "    return root_mean_squared_error(y_real, w[0]*p1 + w[1]*p2 + w[2]*p3)\n",
    "\n",
    "cons = {'type':'eq','fun': lambda w: w.sum()-1.0}\n",
    "bnd  = [(0,1)]*3\n",
    "w0   = [1/3, 1/3, 1/3]\n",
    "\n",
    "res_m0   = sp_minimize(_blend_rmse, w0, args=(vp_lgb_m0, vp_xgb_m0, vp_cat_m0, y_vp_real_m0),\n",
    "                       method='SLSQP', bounds=bnd, constraints=cons)\n",
    "res_rest = sp_minimize(_blend_rmse, w0, args=(vp_lgb_r,  vp_xgb_r,  vp_cat_r,  y_vp_real_rest),\n",
    "                       method='SLSQP', bounds=bnd, constraints=cons)\n",
    "\n",
    "best_w_m0   = res_m0.x\n",
    "best_w_rest = res_rest.x\n",
    "\n",
    "print(f\"  M0   weights [LGB,XGB,CAT]: {np.round(best_w_m0, 4)}  RMSE={res_m0.fun:.4f}\")\n",
    "print(f\"  Rest weights [LGB,XGB,CAT]: {np.round(best_w_rest, 4)}  RMSE={res_rest.fun:.4f}\")\n",
    "\n",
    "json.dump({'m0': best_w_m0.tolist(), 'rest': best_w_rest.tolist()},\n",
    "          open('../models/v8/ensemble_weights.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba1085",
   "metadata": {},
   "source": [
    "## BLOCK E â€” Full Retraining on All Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faeb52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK E: FULL RETRAINING\n",
      "============================================================\n",
      "  Full M0 (nuclear): 14,614   Rest: 109,235\n",
      "  All 6 final models saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60); print(\"BLOCK E: FULL RETRAINING\"); print(\"=\"*60)\n",
    "\n",
    "# Nuclear drop on X_all for M0\n",
    "mask_all_m0   = X_all['market_id'] == 0\n",
    "mask_all_rest = X_all['market_id'] > 0\n",
    "nuc_all       = observed_df['delivery_start'] >= '2024-01-01'\n",
    "\n",
    "X_all_m0   = X_all[mask_all_m0 & nuc_all].drop(columns=['market_id'])\n",
    "y_all_m0   = y_all[(mask_all_m0 & nuc_all).values]\n",
    "X_all_rest = X_all[mask_all_rest]\n",
    "y_all_rest = y_all[mask_all_rest.values]\n",
    "\n",
    "print(f\"  Full M0 (nuclear): {len(X_all_m0):,}   Rest: {len(X_all_rest):,}\")\n",
    "\n",
    "# â”€â”€ LGB final â”€â”€\n",
    "p = {'objective':'huber','metric':'rmse','verbosity':-1,'seed':SEED,'n_jobs':-1}\n",
    "p.update({k.replace('lr','learning_rate').replace('nl','num_leaves'):v\n",
    "          for k,v in s_lgb_m0.best_params.items()})\n",
    "# fix key names\n",
    "_p = {'objective':'huber','metric':'rmse','verbosity':-1,'seed':SEED,'n_jobs':-1,\n",
    "      'learning_rate':s_lgb_m0.best_params['lr'],\n",
    "      'num_leaves':s_lgb_m0.best_params['nl'],\n",
    "      'alpha':s_lgb_m0.best_params['alpha']}\n",
    "lgb_m0_final = lgb.train(_p, lgb.Dataset(X_all_m0, y_all_m0),\n",
    "                          num_boost_round=int(bi_lgb_m0*1.1))\n",
    "\n",
    "_p = {'objective':'huber','metric':'rmse','verbosity':-1,'seed':SEED,'n_jobs':-1,\n",
    "      'learning_rate':s_lgb_rest.best_params['lr'],\n",
    "      'num_leaves':s_lgb_rest.best_params['nl'],\n",
    "      'alpha':s_lgb_rest.best_params['alpha']}\n",
    "cat_idx_rest = [list(X_all_rest.columns).index('market_id')]\n",
    "lgb_rest_final = lgb.train(_p, lgb.Dataset(X_all_rest, y_all_rest, categorical_feature=cat_idx_rest),\n",
    "                            num_boost_round=int(bi_lgb_rest*1.1))\n",
    "\n",
    "lgb_m0_final.save_model('../models/v8/lgb_m0.txt')\n",
    "lgb_rest_final.save_model('../models/v8/lgb_rest.txt')\n",
    "\n",
    "# â”€â”€ XGB final â”€â”€\n",
    "_p = {'tree_method':'hist','objective':'reg:pseudohubererror','eval_metric':'rmse',\n",
    "      'seed':SEED,'n_jobs':-1,\n",
    "      'learning_rate':s_xgb_m0.best_params['lr'],\n",
    "      'max_depth':s_xgb_m0.best_params['md'],\n",
    "      'huber_slope':s_xgb_m0.best_params['alpha']}\n",
    "xgb_m0_final = xgb.train(_p, xgb.DMatrix(X_all_m0, label=y_all_m0),\n",
    "                           num_boost_round=int(bi_xgb_m0*1.1))\n",
    "\n",
    "_p = {'tree_method':'hist','objective':'reg:pseudohubererror','eval_metric':'rmse',\n",
    "      'seed':SEED,'n_jobs':-1,\n",
    "      'learning_rate':s_xgb_rest.best_params['lr'],\n",
    "      'max_depth':s_xgb_rest.best_params['md'],\n",
    "      'huber_slope':s_xgb_rest.best_params['alpha']}\n",
    "xgb_rest_final = xgb.train(_p, xgb.DMatrix(X_all_rest, label=y_all_rest),\n",
    "                             num_boost_round=int(bi_xgb_rest*1.1))\n",
    "\n",
    "xgb_m0_final.save_model('../models/v8/xgb_m0.json')\n",
    "xgb_rest_final.save_model('../models/v8/xgb_rest.json')\n",
    "\n",
    "# â”€â”€ CAT final â”€â”€\n",
    "_p = {'loss_function':'Huber:delta=1.5','task_type':'CPU','random_seed':SEED,'verbose':False,\n",
    "      'learning_rate':s_cat_m0.best_params['lr'],\n",
    "      'depth':s_cat_m0.best_params['depth'],\n",
    "      'iterations':int(bi_cat_m0*1.1)}\n",
    "cat_m0_final = CatBoostRegressor(**_p)\n",
    "cat_m0_final.fit(X_all_m0, y_all_m0)\n",
    "\n",
    "_p = {'loss_function':'Huber:delta=1.5','task_type':'CPU','random_seed':SEED,'verbose':False,\n",
    "      'learning_rate':s_cat_rest.best_params['lr'],\n",
    "      'depth':s_cat_rest.best_params['depth'],\n",
    "      'iterations':int(bi_cat_rest*1.1)}\n",
    "cat_rest_final = CatBoostRegressor(**_p)\n",
    "cat_rest_final.fit(Pool(X_all_rest, y_all_rest,\n",
    "                        cat_features=[list(X_all_rest.columns).index('market_id')]))\n",
    "\n",
    "cat_m0_final.save_model('../models/v8/cat_m0.cbm')\n",
    "cat_rest_final.save_model('../models/v8/cat_rest.cbm')\n",
    "\n",
    "print(\"  All 6 final models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ae704",
   "metadata": {},
   "source": [
    "## BLOCK F â€” Ensemble Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdbb5dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK F: ENSEMBLE INFERENCE\n",
      "============================================================\n",
      "âœ… Saved: ../submissions/submission_v8_ensemble.csv\n",
      "   Mean:   31.8885\n",
      "   Std:    30.0919\n",
      "   Min:    -122.7772\n",
      "   Max:    359.4650\n",
      "   Median: 27.3252\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60); print(\"BLOCK F: ENSEMBLE INFERENCE\"); print(\"=\"*60)\n",
    "\n",
    "mask_te_m0   = X_test['market_id'] == 0\n",
    "mask_te_rest = X_test['market_id'] > 0\n",
    "\n",
    "Xt_m0   = X_test[mask_te_m0].drop(columns=['market_id'])\n",
    "Xt_rest = X_test[mask_te_rest]\n",
    "\n",
    "# Predictions (arcsinh â†’ sinh)\n",
    "lg_m0 = np.sinh(np.clip(lgb_m0_final.predict(Xt_m0), -20, 20))\n",
    "lg_r  = np.sinh(np.clip(lgb_rest_final.predict(Xt_rest), -20, 20))\n",
    "xg_m0 = np.sinh(np.clip(xgb_m0_final.predict(xgb.DMatrix(Xt_m0)), -20, 20))\n",
    "xg_r  = np.sinh(np.clip(xgb_rest_final.predict(xgb.DMatrix(Xt_rest)), -20, 20))\n",
    "ct_m0 = np.sinh(np.clip(cat_m0_final.predict(Xt_m0), -20, 20))\n",
    "ct_r  = np.sinh(np.clip(cat_rest_final.predict(Xt_rest), -20, 20))\n",
    "\n",
    "# Blend using Scipy-optimized weights\n",
    "final_preds = np.empty(len(X_test))\n",
    "final_preds[mask_te_m0.values]   = best_w_m0[0]*lg_m0 + best_w_m0[1]*xg_m0 + best_w_m0[2]*ct_m0\n",
    "final_preds[mask_te_rest.values] = best_w_rest[0]*lg_r + best_w_rest[1]*xg_r + best_w_rest[2]*ct_r\n",
    "\n",
    "# Build submission\n",
    "pred_df = pd.DataFrame({\"id\": test_df[\"id\"].values, \"target\": final_preds})\n",
    "submission = sample_sub[[\"id\"]].merge(pred_df, on=\"id\", how=\"left\")\n",
    "\n",
    "assert len(submission) == len(sample_sub)\n",
    "assert submission[\"target\"].isna().sum() == 0\n",
    "assert (submission[\"id\"] == sample_sub[\"id\"]).all()\n",
    "\n",
    "path = \"../submissions/submission_v8_ensemble.csv\"\n",
    "submission.to_csv(path, index=False)\n",
    "\n",
    "print(f\"âœ… Saved: {path}\")\n",
    "print(f\"   Mean:   {submission['target'].mean():.4f}\")\n",
    "print(f\"   Std:    {submission['target'].std():.4f}\")\n",
    "print(f\"   Min:    {submission['target'].min():.4f}\")\n",
    "print(f\"   Max:    {submission['target'].max():.4f}\")\n",
    "print(f\"   Median: {submission['target'].median():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitor_kuas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
